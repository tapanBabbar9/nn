{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05cf3ea-836e-438c-8571-3059933eb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM from scratch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WeightInitializer:\n",
    "    def __init__(self, method='random'):\n",
    "        self.method = method\n",
    "\n",
    "    def initialize(self, shape):\n",
    "        if self.method == 'random':\n",
    "            return np.random.randn(*shape)\n",
    "        elif self.method == 'xavier':\n",
    "            return np.random.randn(*shape) / np.sqrt(shape[0])\n",
    "        elif self.method == 'he':\n",
    "            return np.random.randn(*shape) * np.sqrt(2 / shape[0])\n",
    "        elif self.method == 'uniform':\n",
    "            return np.random.uniform(-1, 1, shape)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown initialization method: {self.method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c9bd34-de40-4f98-a917-55f9fe85d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotManager:\n",
    "    def __init__(self):\n",
    "        # Create a single axis for plotting\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4))  # One plot only\n",
    "\n",
    "    def plot_losses(self, train_losses, val_losses):\n",
    "        self.ax.plot(train_losses, label='Training Loss')\n",
    "        self.ax.plot(val_losses, label='Validation Loss')\n",
    "        self.ax.set_title('Training and Validation Losses')\n",
    "        self.ax.set_xlabel('Epoch')\n",
    "        self.ax.set_ylabel('Loss')\n",
    "        self.ax.legend()\n",
    "\n",
    "    def show_plots(self):\n",
    "        plt.tight_layout()\n",
    "        plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6f3816-fcce-44bf-89b2-d73213953b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        patience (int): Number of epochs to wait before stopping the training.\n",
    "        verbose (bool): If True, prints a message for each epoch where the loss\n",
    "                        does not improve.\n",
    "        delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Determines if the model should stop training.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): The loss of the model on the validation set.\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afa6ccc-d9f7-47ca-b15a-55d39b2a9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \"\"\"\n",
    "    Long Short-Term Memory (LSTM) network.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_size: int, dimensionality of input space\n",
    "    - hidden_size: int, number of LSTM units\n",
    "    - output_size: int, dimensionality of output space\n",
    "    - init_method: str, weight initialization method (default: 'xavier')\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, init_method='xavier'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_initializer = WeightInitializer(method=init_method)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.wf = self.weight_initializer.initialize((hidden_size, hidden_size + input_size))\n",
    "        self.wi = self.weight_initializer.initialize((hidden_size, hidden_size + input_size))\n",
    "        self.wo = self.weight_initializer.initialize((hidden_size, hidden_size + input_size))\n",
    "        self.wc = self.weight_initializer.initialize((hidden_size, hidden_size + input_size))\n",
    "\n",
    "        # Initialize biases\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize output layer weights and biases\n",
    "        self.why = self.weight_initializer.initialize((output_size, hidden_size))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        - z: np.ndarray, input to the activation function\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray, output of the activation function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def dsigmoid(y):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y: np.ndarray, output of the sigmoid activation function\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, derivative of the sigmoid function\n",
    "        \"\"\"\n",
    "        return y * (1 - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def dtanh(y):\n",
    "        \"\"\"\n",
    "        Derivative of the hyperbolic tangent activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y: np.ndarray, output of the hyperbolic tangent activation function\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, derivative of the hyperbolic tangent function\n",
    "        \"\"\"\n",
    "        return 1 - y * y\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM network.\n",
    "\n",
    "        Parameters:\n",
    "        - x: np.ndarray, input to the network\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, output of the network\n",
    "        - list, caches containing intermediate values for backpropagation\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        h_prev = np.zeros((self.hidden_size, 1))\n",
    "        c_prev = np.zeros((self.hidden_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "\n",
    "        for t in range(x.shape[0]):\n",
    "            x_t = x[t].reshape(-1, 1)\n",
    "            combined = np.vstack((h_prev, x_t))\n",
    "            \n",
    "            f = self.sigmoid(np.dot(self.wf, combined) + self.bf)\n",
    "            i = self.sigmoid(np.dot(self.wi, combined) + self.bi)\n",
    "            o = self.sigmoid(np.dot(self.wo, combined) + self.bo)\n",
    "            c_ = np.tanh(np.dot(self.wc, combined) + self.bc)\n",
    "            \n",
    "            c = f * c_prev + i * c_\n",
    "            h = o * np.tanh(c)\n",
    "\n",
    "            cache = (h_prev, c_prev, f, i, o, c_, x_t, combined, c, h)\n",
    "            caches.append(cache)\n",
    "\n",
    "            h_prev, c_prev = h, c\n",
    "\n",
    "        y = np.dot(self.why, h) + self.by\n",
    "        return y, caches\n",
    "\n",
    "    def backward(self, dy, caches, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Backward pass through the LSTM network.\n",
    "\n",
    "        Parameters:\n",
    "        - dy: np.ndarray, gradient of the loss with respect to the output\n",
    "        - caches: list, caches from the forward pass\n",
    "        - clip_value: float, value to clip gradients to (default: 1.0)\n",
    "\n",
    "        Returns:\n",
    "        - tuple, gradients of the loss with respect to the parameters\n",
    "        \"\"\"\n",
    "        dWf, dWi, dWo, dWc = [np.zeros_like(w) for w in (self.wf, self.wi, self.wo, self.wc)]\n",
    "        dbf, dbi, dbo, dbc = [np.zeros_like(b) for b in (self.bf, self.bi, self.bo, self.bc)]\n",
    "        dWhy = np.zeros_like(self.why)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        # Ensure dy is reshaped to match output size\n",
    "        dy = dy.reshape(self.output_size, -1)\n",
    "        dh_next = np.zeros((self.hidden_size, 1))  # shape must match hidden_size\n",
    "        dc_next = np.zeros_like(dh_next)\n",
    "\n",
    "        for cache in reversed(caches):\n",
    "            h_prev, c_prev, f, i, o, c_, x_t, combined, c, h = cache\n",
    "\n",
    "            # Add gradient from next step to current output gradient\n",
    "            dh = np.dot(self.why.T, dy) + dh_next\n",
    "            dc = dc_next + (dh * o * self.dtanh(np.tanh(c)))\n",
    "\n",
    "            df = dc * c_prev * self.dsigmoid(f)\n",
    "            di = dc * c_ * self.dsigmoid(i)\n",
    "            do = dh * self.dtanh(np.tanh(c))\n",
    "            dc_ = dc * i * self.dtanh(c_)\n",
    "\n",
    "            dcombined_f = np.dot(self.wf.T, df)\n",
    "            dcombined_i = np.dot(self.wi.T, di)\n",
    "            dcombined_o = np.dot(self.wo.T, do)\n",
    "            dcombined_c = np.dot(self.wc.T, dc_)\n",
    "\n",
    "            dcombined = dcombined_f + dcombined_i + dcombined_o + dcombined_c\n",
    "            dh_next = dcombined[:self.hidden_size]\n",
    "            dc_next = f * dc\n",
    "\n",
    "            dWf += np.dot(df, combined.T)\n",
    "            dWi += np.dot(di, combined.T)\n",
    "            dWo += np.dot(do, combined.T)\n",
    "            dWc += np.dot(dc_, combined.T)\n",
    "\n",
    "            dbf += df.sum(axis=1, keepdims=True)\n",
    "            dbi += di.sum(axis=1, keepdims=True)\n",
    "            dbo += do.sum(axis=1, keepdims=True)\n",
    "            dbc += dc_.sum(axis=1, keepdims=True)\n",
    "\n",
    "        dWhy += np.dot(dy, h.T)\n",
    "        dby += dy\n",
    "\n",
    "        gradients = (dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby)\n",
    "\n",
    "        # Gradient clipping\n",
    "        for i in range(len(gradients)):\n",
    "            np.clip(gradients[i], -clip_value, clip_value, out=gradients[i])\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the parameters of the network using the gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - grads: tuple, gradients of the loss with respect to the parameters\n",
    "        - learning_rate: float, learning rate\n",
    "        \"\"\"\n",
    "        dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWhy, dby = grads\n",
    "\n",
    "        self.wf -= learning_rate * dWf\n",
    "        self.wi -= learning_rate * dWi\n",
    "        self.wo -= learning_rate * dWo\n",
    "        self.wc -= learning_rate * dWc\n",
    "\n",
    "        self.bf -= learning_rate * dbf\n",
    "        self.bi -= learning_rate * dbi\n",
    "        self.bo -= learning_rate * dbo\n",
    "        self.bc -= learning_rate * dbc\n",
    "\n",
    "        self.why -= learning_rate * dWhy\n",
    "        self.by -= learning_rate * dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b16544d-f15a-4104-89a0-ac3351c78d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the LSTM network.\n",
    "\n",
    "    Parameters:\n",
    "    - model: LSTM, the LSTM network to train\n",
    "    - learning_rate: float, learning rate for the optimizer\n",
    "    - patience: int, number of epochs to wait before early stopping\n",
    "    - verbose: bool, whether to print training information\n",
    "    - delta: float, minimum change in validation loss to qualify as an improvement\n",
    "    \"\"\"\n",
    "    def __init__(self, model, learning_rate=0.01, patience=7, verbose=True, delta=0):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.early_stopping = EarlyStopping(patience, verbose, delta)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=10, batch_size=1, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Train the LSTM network.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: np.ndarray, training data\n",
    "        - y_train: np.ndarray, training labels\n",
    "        - X_val: np.ndarray, validation data\n",
    "        - y_val: np.ndarray, validation labels\n",
    "        - epochs: int, number of training epochs\n",
    "        - batch_size: int, size of mini-batches\n",
    "        - clip_value: float, value to clip gradients to\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = []\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_X = X_train[i:i + batch_size]\n",
    "                batch_y = y_train[i:i + batch_size]\n",
    "                losses = []\n",
    "                \n",
    "                for x, y_true in zip(batch_X, batch_y):\n",
    "                    y_pred, caches = self.model.forward(x)\n",
    "                    loss = self.compute_loss(y_pred, y_true.reshape(-1, 1))\n",
    "                    losses.append(loss)\n",
    "                    \n",
    "                    # Backpropagation to get gradients\n",
    "                    dy = y_pred - y_true.reshape(-1, 1)\n",
    "                    grads = self.model.backward(dy, caches, clip_value=clip_value)\n",
    "                    self.model.update_params(grads, self.learning_rate)\n",
    "\n",
    "                batch_loss = np.mean(losses)\n",
    "                epoch_losses.append(batch_loss)\n",
    "\n",
    "            avg_epoch_loss = np.mean(epoch_losses)\n",
    "            self.train_losses.append(avg_epoch_loss)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_loss = self.validate(X_val, y_val)\n",
    "                self.val_losses.append(val_loss)\n",
    "                print(f'Epoch {epoch + 1}/{epochs} - Loss: {avg_epoch_loss:.5f}, Val Loss: {val_loss:.5f}')\n",
    "                \n",
    "                # Check early stopping condition\n",
    "                self.early_stopping(val_loss)\n",
    "                if self.early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}/{epochs} - Loss: {avg_epoch_loss:.5f}')\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute mean squared error loss.\n",
    "        \"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def validate(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Validate the model on a separate set of data.\n",
    "        \"\"\"\n",
    "        val_losses = []\n",
    "        for x, y_true in zip(X_val, y_val):\n",
    "            y_pred, _ = self.model.forward(x)\n",
    "            loss = self.compute_loss(y_pred, y_true.reshape(-1, 1))\n",
    "            val_losses.append(loss)\n",
    "        return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef60ed28-33a2-4c23-ab4f-62dfa96a24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for time series data.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker: str, stock ticker symbol\n",
    "    - start_date: str, start date for data retrieval\n",
    "    - end_date: str, end date for data retrieval\n",
    "    - look_back: int, number of previous time steps to include in each sample\n",
    "    - train_size: float, proportion of data to use for training\n",
    "    \"\"\"\n",
    "    def __init__(self, start_date, end_date, look_back=1, train_size=0.67):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.look_back = look_back\n",
    "        self.train_size = train_size\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load stock data.\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray, training data\n",
    "        - np.ndarray, testing data\n",
    "        \"\"\"\n",
    "        df = pd.read_csv('Google_Stock_Train (2010-2022).csv')\n",
    "        df = df[(df['Date'] >= self.start_date) & (df['Date'] <= self.end_date)]\n",
    "        df = df.sort_index()\n",
    "        df = df.loc[self.start_date:self.end_date]\n",
    "        df = df[['Close']].astype(float)  # Use closing price\n",
    "        df = self.MinMaxScaler(df.values)  # Convert DataFrame to numpy array\n",
    "        train_size = int(len(df) * self.train_size)\n",
    "        train, test = df[0:train_size,:], df[train_size:len(df),:]\n",
    "        return train, test\n",
    "    \n",
    "    def MinMaxScaler(self, data):\n",
    "        \"\"\"\n",
    "        Min-max scaling of the data.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: np.ndarray, input data\n",
    "        \"\"\"\n",
    "        numerator = data - np.min(data, 0)\n",
    "        denominator = np.max(data, 0) - np.min(data, 0)\n",
    "        return numerator / (denominator + 1e-7)\n",
    "\n",
    "    def create_dataset(self, dataset):\n",
    "        \"\"\"\n",
    "        Create the dataset for time series prediction.\n",
    "\n",
    "        Parameters:\n",
    "        - dataset: np.ndarray, input data\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, input data\n",
    "        - np.ndarray, output data\n",
    "        \"\"\"\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-self.look_back):\n",
    "            a = dataset[i:(i + self.look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + self.look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    def get_train_test(self):\n",
    "        \"\"\"\n",
    "        Get the training and testing data.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray, training input\n",
    "        - np.ndarray, training output\n",
    "        - np.ndarray, testing input\n",
    "        - np.ndarray, testing output\n",
    "        \"\"\"\n",
    "        train, test = self.load_data()\n",
    "        trainX, trainY = self.create_dataset(train)\n",
    "        testX, testY = self.create_dataset(test)\n",
    "        return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deae78f3-1a34-45c3-88c3-2479efeb5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "dataset = TimeSeriesDataset( '2010-1-1', '2020-12-31', look_back=1)\n",
    "trainX, trainY, testX, testY = dataset.get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2af211a-b14a-4e18-8130-eeeead08c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b54ff28-ee1d-4899-9f75-23a27d18fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1  # Number of previous time steps to include in each sample\n",
    "hidden_size = 256  # Number of LSTM units\n",
    "output_size = 1  # Dimensionality of the output space\n",
    "\n",
    "lstm = LSTM(input_size=1, hidden_size=hidden_size, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5877d83-99a0-44d1-894d-3d07dc839620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - Loss: 0.48420, Val Loss: 1.25261\n",
      "Epoch 2/1000 - Loss: 0.42614, Val Loss: 1.12504\n",
      "Epoch 3/1000 - Loss: 0.37687, Val Loss: 1.01331\n",
      "Epoch 4/1000 - Loss: 0.33507, Val Loss: 0.91529\n",
      "Epoch 5/1000 - Loss: 0.29960, Val Loss: 0.82915\n",
      "Epoch 6/1000 - Loss: 0.26948, Val Loss: 0.75329\n",
      "Epoch 7/1000 - Loss: 0.24388, Val Loss: 0.68636\n",
      "Epoch 8/1000 - Loss: 0.22210, Val Loss: 0.62719\n",
      "Epoch 9/1000 - Loss: 0.20356, Val Loss: 0.57478\n",
      "Epoch 10/1000 - Loss: 0.18775, Val Loss: 0.52825\n",
      "Epoch 11/1000 - Loss: 0.17426, Val Loss: 0.48687\n",
      "Epoch 12/1000 - Loss: 0.16273, Val Loss: 0.44997\n",
      "Epoch 13/1000 - Loss: 0.15285, Val Loss: 0.41702\n",
      "Epoch 14/1000 - Loss: 0.14438, Val Loss: 0.38752\n",
      "Epoch 15/1000 - Loss: 0.13710, Val Loss: 0.36106\n",
      "Epoch 16/1000 - Loss: 0.13082, Val Loss: 0.33728\n",
      "Epoch 17/1000 - Loss: 0.12540, Val Loss: 0.31586\n",
      "Epoch 18/1000 - Loss: 0.12070, Val Loss: 0.29653\n",
      "Epoch 19/1000 - Loss: 0.11661, Val Loss: 0.27905\n",
      "Epoch 20/1000 - Loss: 0.11304, Val Loss: 0.26320\n",
      "Epoch 21/1000 - Loss: 0.10991, Val Loss: 0.24881\n",
      "Epoch 22/1000 - Loss: 0.10715, Val Loss: 0.23571\n",
      "Epoch 23/1000 - Loss: 0.10472, Val Loss: 0.22376\n",
      "Epoch 24/1000 - Loss: 0.10255, Val Loss: 0.21283\n",
      "Epoch 25/1000 - Loss: 0.10061, Val Loss: 0.20283\n",
      "Epoch 26/1000 - Loss: 0.09886, Val Loss: 0.19365\n",
      "Epoch 27/1000 - Loss: 0.09728, Val Loss: 0.18520\n",
      "Epoch 28/1000 - Loss: 0.09584, Val Loss: 0.17741\n",
      "Epoch 29/1000 - Loss: 0.09452, Val Loss: 0.17022\n",
      "Epoch 30/1000 - Loss: 0.09331, Val Loss: 0.16356\n",
      "Epoch 31/1000 - Loss: 0.09218, Val Loss: 0.15739\n",
      "Epoch 32/1000 - Loss: 0.09113, Val Loss: 0.15165\n",
      "Epoch 33/1000 - Loss: 0.09014, Val Loss: 0.14630\n",
      "Epoch 34/1000 - Loss: 0.08920, Val Loss: 0.14132\n",
      "Epoch 35/1000 - Loss: 0.08832, Val Loss: 0.13665\n",
      "Epoch 36/1000 - Loss: 0.08747, Val Loss: 0.13229\n",
      "Epoch 37/1000 - Loss: 0.08666, Val Loss: 0.12818\n",
      "Epoch 38/1000 - Loss: 0.08588, Val Loss: 0.12433\n",
      "Epoch 39/1000 - Loss: 0.08513, Val Loss: 0.12069\n",
      "Epoch 40/1000 - Loss: 0.08440, Val Loss: 0.11726\n",
      "Epoch 41/1000 - Loss: 0.08369, Val Loss: 0.11401\n",
      "Epoch 42/1000 - Loss: 0.08300, Val Loss: 0.11093\n",
      "Epoch 43/1000 - Loss: 0.08233, Val Loss: 0.10801\n",
      "Epoch 44/1000 - Loss: 0.08167, Val Loss: 0.10523\n",
      "Epoch 45/1000 - Loss: 0.08103, Val Loss: 0.10258\n",
      "Epoch 46/1000 - Loss: 0.08040, Val Loss: 0.10005\n",
      "Epoch 47/1000 - Loss: 0.07978, Val Loss: 0.09764\n",
      "Epoch 48/1000 - Loss: 0.07917, Val Loss: 0.09532\n",
      "Epoch 49/1000 - Loss: 0.07856, Val Loss: 0.09310\n",
      "Epoch 50/1000 - Loss: 0.07797, Val Loss: 0.09097\n",
      "Epoch 51/1000 - Loss: 0.07739, Val Loss: 0.08892\n",
      "Epoch 52/1000 - Loss: 0.07681, Val Loss: 0.08694\n",
      "Epoch 53/1000 - Loss: 0.07624, Val Loss: 0.08504\n",
      "Epoch 54/1000 - Loss: 0.07568, Val Loss: 0.08320\n",
      "Epoch 55/1000 - Loss: 0.07512, Val Loss: 0.08142\n",
      "Epoch 56/1000 - Loss: 0.07457, Val Loss: 0.07970\n",
      "Epoch 57/1000 - Loss: 0.07402, Val Loss: 0.07804\n",
      "Epoch 58/1000 - Loss: 0.07349, Val Loss: 0.07642\n",
      "Epoch 59/1000 - Loss: 0.07295, Val Loss: 0.07485\n",
      "Epoch 60/1000 - Loss: 0.07243, Val Loss: 0.07332\n",
      "Epoch 61/1000 - Loss: 0.07191, Val Loss: 0.07184\n",
      "Epoch 62/1000 - Loss: 0.07139, Val Loss: 0.07039\n",
      "Epoch 63/1000 - Loss: 0.07088, Val Loss: 0.06899\n",
      "Epoch 64/1000 - Loss: 0.07038, Val Loss: 0.06761\n",
      "Epoch 65/1000 - Loss: 0.06988, Val Loss: 0.06627\n",
      "Epoch 66/1000 - Loss: 0.06938, Val Loss: 0.06497\n",
      "Epoch 67/1000 - Loss: 0.06889, Val Loss: 0.06369\n",
      "Epoch 68/1000 - Loss: 0.06841, Val Loss: 0.06244\n",
      "Epoch 69/1000 - Loss: 0.06793, Val Loss: 0.06122\n",
      "Epoch 70/1000 - Loss: 0.06745, Val Loss: 0.06002\n",
      "Epoch 71/1000 - Loss: 0.06698, Val Loss: 0.05885\n",
      "Epoch 72/1000 - Loss: 0.06651, Val Loss: 0.05771\n",
      "Epoch 73/1000 - Loss: 0.06605, Val Loss: 0.05659\n",
      "Epoch 74/1000 - Loss: 0.06560, Val Loss: 0.05549\n",
      "Epoch 75/1000 - Loss: 0.06514, Val Loss: 0.05441\n",
      "Epoch 76/1000 - Loss: 0.06470, Val Loss: 0.05335\n",
      "Epoch 77/1000 - Loss: 0.06425, Val Loss: 0.05231\n",
      "Epoch 78/1000 - Loss: 0.06381, Val Loss: 0.05129\n",
      "Epoch 79/1000 - Loss: 0.06338, Val Loss: 0.05029\n",
      "Epoch 80/1000 - Loss: 0.06295, Val Loss: 0.04931\n",
      "Epoch 81/1000 - Loss: 0.06252, Val Loss: 0.04835\n",
      "Epoch 82/1000 - Loss: 0.06210, Val Loss: 0.04740\n",
      "Epoch 83/1000 - Loss: 0.06169, Val Loss: 0.04647\n",
      "Epoch 84/1000 - Loss: 0.06127, Val Loss: 0.04556\n",
      "Epoch 85/1000 - Loss: 0.06086, Val Loss: 0.04466\n",
      "Epoch 86/1000 - Loss: 0.06046, Val Loss: 0.04377\n",
      "Epoch 87/1000 - Loss: 0.06006, Val Loss: 0.04291\n",
      "Epoch 88/1000 - Loss: 0.05966, Val Loss: 0.04205\n",
      "Epoch 89/1000 - Loss: 0.05927, Val Loss: 0.04121\n",
      "Epoch 90/1000 - Loss: 0.05888, Val Loss: 0.04039\n",
      "Epoch 91/1000 - Loss: 0.05849, Val Loss: 0.03958\n",
      "Epoch 92/1000 - Loss: 0.05811, Val Loss: 0.03878\n",
      "Epoch 93/1000 - Loss: 0.05773, Val Loss: 0.03800\n",
      "Epoch 94/1000 - Loss: 0.05736, Val Loss: 0.03723\n",
      "Epoch 95/1000 - Loss: 0.05699, Val Loss: 0.03647\n",
      "Epoch 96/1000 - Loss: 0.05662, Val Loss: 0.03572\n",
      "Epoch 97/1000 - Loss: 0.05626, Val Loss: 0.03499\n",
      "Epoch 98/1000 - Loss: 0.05590, Val Loss: 0.03426\n",
      "Epoch 99/1000 - Loss: 0.05555, Val Loss: 0.03355\n",
      "Epoch 100/1000 - Loss: 0.05520, Val Loss: 0.03286\n",
      "Epoch 101/1000 - Loss: 0.05485, Val Loss: 0.03217\n",
      "Epoch 102/1000 - Loss: 0.05450, Val Loss: 0.03149\n",
      "Epoch 103/1000 - Loss: 0.05416, Val Loss: 0.03083\n",
      "Epoch 104/1000 - Loss: 0.05382, Val Loss: 0.03017\n",
      "Epoch 105/1000 - Loss: 0.05349, Val Loss: 0.02953\n",
      "Epoch 106/1000 - Loss: 0.05316, Val Loss: 0.02890\n",
      "Epoch 107/1000 - Loss: 0.05283, Val Loss: 0.02828\n",
      "Epoch 108/1000 - Loss: 0.05251, Val Loss: 0.02766\n",
      "Epoch 109/1000 - Loss: 0.05218, Val Loss: 0.02706\n",
      "Epoch 110/1000 - Loss: 0.05187, Val Loss: 0.02647\n",
      "Epoch 111/1000 - Loss: 0.05155, Val Loss: 0.02589\n",
      "Epoch 112/1000 - Loss: 0.05124, Val Loss: 0.02532\n",
      "Epoch 113/1000 - Loss: 0.05093, Val Loss: 0.02475\n",
      "Epoch 114/1000 - Loss: 0.05062, Val Loss: 0.02420\n",
      "Epoch 115/1000 - Loss: 0.05032, Val Loss: 0.02366\n",
      "Epoch 116/1000 - Loss: 0.05002, Val Loss: 0.02312\n",
      "Epoch 117/1000 - Loss: 0.04973, Val Loss: 0.02260\n",
      "Epoch 118/1000 - Loss: 0.04943, Val Loss: 0.02208\n",
      "Epoch 119/1000 - Loss: 0.04914, Val Loss: 0.02157\n",
      "Epoch 120/1000 - Loss: 0.04885, Val Loss: 0.02107\n",
      "Epoch 121/1000 - Loss: 0.04857, Val Loss: 0.02058\n",
      "Epoch 122/1000 - Loss: 0.04829, Val Loss: 0.02010\n",
      "Epoch 123/1000 - Loss: 0.04801, Val Loss: 0.01962\n",
      "Epoch 124/1000 - Loss: 0.04773, Val Loss: 0.01916\n",
      "Epoch 125/1000 - Loss: 0.04746, Val Loss: 0.01870\n",
      "Epoch 126/1000 - Loss: 0.04719, Val Loss: 0.01825\n",
      "Epoch 127/1000 - Loss: 0.04692, Val Loss: 0.01781\n",
      "Epoch 128/1000 - Loss: 0.04666, Val Loss: 0.01737\n",
      "Epoch 129/1000 - Loss: 0.04639, Val Loss: 0.01695\n",
      "Epoch 130/1000 - Loss: 0.04613, Val Loss: 0.01653\n",
      "Epoch 131/1000 - Loss: 0.04587, Val Loss: 0.01612\n",
      "Epoch 132/1000 - Loss: 0.04562, Val Loss: 0.01572\n",
      "Epoch 133/1000 - Loss: 0.04537, Val Loss: 0.01532\n",
      "Epoch 134/1000 - Loss: 0.04512, Val Loss: 0.01493\n",
      "Epoch 135/1000 - Loss: 0.04487, Val Loss: 0.01455\n",
      "Epoch 136/1000 - Loss: 0.04463, Val Loss: 0.01417\n",
      "Epoch 137/1000 - Loss: 0.04438, Val Loss: 0.01381\n",
      "Epoch 138/1000 - Loss: 0.04414, Val Loss: 0.01345\n",
      "Epoch 139/1000 - Loss: 0.04391, Val Loss: 0.01309\n",
      "Epoch 140/1000 - Loss: 0.04367, Val Loss: 0.01274\n",
      "Epoch 141/1000 - Loss: 0.04344, Val Loss: 0.01240\n",
      "Epoch 142/1000 - Loss: 0.04321, Val Loss: 0.01207\n",
      "Epoch 143/1000 - Loss: 0.04298, Val Loss: 0.01174\n",
      "Epoch 144/1000 - Loss: 0.04275, Val Loss: 0.01142\n",
      "Epoch 145/1000 - Loss: 0.04253, Val Loss: 0.01110\n",
      "Epoch 146/1000 - Loss: 0.04231, Val Loss: 0.01080\n",
      "Epoch 147/1000 - Loss: 0.04209, Val Loss: 0.01049\n",
      "Epoch 148/1000 - Loss: 0.04187, Val Loss: 0.01020\n",
      "Epoch 149/1000 - Loss: 0.04166, Val Loss: 0.00991\n",
      "Epoch 150/1000 - Loss: 0.04144, Val Loss: 0.00962\n",
      "Epoch 151/1000 - Loss: 0.04123, Val Loss: 0.00934\n",
      "Epoch 152/1000 - Loss: 0.04103, Val Loss: 0.00907\n",
      "Epoch 153/1000 - Loss: 0.04082, Val Loss: 0.00880\n",
      "Epoch 154/1000 - Loss: 0.04062, Val Loss: 0.00854\n",
      "Epoch 155/1000 - Loss: 0.04041, Val Loss: 0.00828\n",
      "Epoch 156/1000 - Loss: 0.04021, Val Loss: 0.00803\n",
      "Epoch 157/1000 - Loss: 0.04001, Val Loss: 0.00779\n",
      "Epoch 158/1000 - Loss: 0.03982, Val Loss: 0.00755\n",
      "Epoch 159/1000 - Loss: 0.03962, Val Loss: 0.00731\n",
      "Epoch 160/1000 - Loss: 0.03943, Val Loss: 0.00708\n",
      "Epoch 161/1000 - Loss: 0.03924, Val Loss: 0.00686\n",
      "Epoch 162/1000 - Loss: 0.03905, Val Loss: 0.00664\n",
      "Epoch 163/1000 - Loss: 0.03887, Val Loss: 0.00642\n",
      "Epoch 164/1000 - Loss: 0.03868, Val Loss: 0.00622\n",
      "Epoch 165/1000 - Loss: 0.03850, Val Loss: 0.00601\n",
      "Epoch 166/1000 - Loss: 0.03832, Val Loss: 0.00581\n",
      "Epoch 167/1000 - Loss: 0.03814, Val Loss: 0.00562\n",
      "Epoch 168/1000 - Loss: 0.03796, Val Loss: 0.00543\n",
      "Epoch 169/1000 - Loss: 0.03779, Val Loss: 0.00524\n",
      "Epoch 170/1000 - Loss: 0.03761, Val Loss: 0.00506\n",
      "Epoch 171/1000 - Loss: 0.03744, Val Loss: 0.00488\n",
      "Epoch 172/1000 - Loss: 0.03727, Val Loss: 0.00471\n",
      "Epoch 173/1000 - Loss: 0.03710, Val Loss: 0.00454\n",
      "Epoch 174/1000 - Loss: 0.03693, Val Loss: 0.00438\n",
      "Epoch 175/1000 - Loss: 0.03677, Val Loss: 0.00422\n",
      "Epoch 176/1000 - Loss: 0.03661, Val Loss: 0.00407\n",
      "Epoch 177/1000 - Loss: 0.03644, Val Loss: 0.00391\n",
      "Epoch 178/1000 - Loss: 0.03628, Val Loss: 0.00377\n",
      "Epoch 179/1000 - Loss: 0.03612, Val Loss: 0.00363\n",
      "Epoch 180/1000 - Loss: 0.03597, Val Loss: 0.00349\n",
      "Epoch 181/1000 - Loss: 0.03581, Val Loss: 0.00335\n",
      "Epoch 182/1000 - Loss: 0.03566, Val Loss: 0.00322\n",
      "Epoch 183/1000 - Loss: 0.03550, Val Loss: 0.00309\n",
      "Epoch 184/1000 - Loss: 0.03535, Val Loss: 0.00297\n",
      "Epoch 185/1000 - Loss: 0.03520, Val Loss: 0.00285\n",
      "Epoch 186/1000 - Loss: 0.03506, Val Loss: 0.00274\n",
      "Epoch 187/1000 - Loss: 0.03491, Val Loss: 0.00262\n",
      "Epoch 188/1000 - Loss: 0.03476, Val Loss: 0.00252\n",
      "Epoch 189/1000 - Loss: 0.03462, Val Loss: 0.00241\n",
      "Epoch 190/1000 - Loss: 0.03448, Val Loss: 0.00231\n",
      "Epoch 191/1000 - Loss: 0.03434, Val Loss: 0.00221\n",
      "Epoch 192/1000 - Loss: 0.03420, Val Loss: 0.00212\n",
      "Epoch 193/1000 - Loss: 0.03406, Val Loss: 0.00203\n",
      "Epoch 194/1000 - Loss: 0.03392, Val Loss: 0.00194\n",
      "Epoch 195/1000 - Loss: 0.03379, Val Loss: 0.00185\n",
      "Epoch 196/1000 - Loss: 0.03366, Val Loss: 0.00177\n",
      "Epoch 197/1000 - Loss: 0.03352, Val Loss: 0.00170\n",
      "Epoch 198/1000 - Loss: 0.03339, Val Loss: 0.00162\n",
      "Epoch 199/1000 - Loss: 0.03326, Val Loss: 0.00155\n",
      "Epoch 200/1000 - Loss: 0.03313, Val Loss: 0.00148\n",
      "Epoch 201/1000 - Loss: 0.03301, Val Loss: 0.00141\n",
      "Epoch 202/1000 - Loss: 0.03288, Val Loss: 0.00135\n",
      "Epoch 203/1000 - Loss: 0.03276, Val Loss: 0.00129\n",
      "Epoch 204/1000 - Loss: 0.03263, Val Loss: 0.00123\n",
      "Epoch 205/1000 - Loss: 0.03251, Val Loss: 0.00118\n",
      "Epoch 206/1000 - Loss: 0.03239, Val Loss: 0.00113\n",
      "Epoch 207/1000 - Loss: 0.03227, Val Loss: 0.00108\n",
      "Epoch 208/1000 - Loss: 0.03215, Val Loss: 0.00103\n",
      "Epoch 209/1000 - Loss: 0.03203, Val Loss: 0.00099\n",
      "Epoch 210/1000 - Loss: 0.03192, Val Loss: 0.00095\n",
      "Epoch 211/1000 - Loss: 0.03180, Val Loss: 0.00091\n",
      "Epoch 212/1000 - Loss: 0.03169, Val Loss: 0.00088\n",
      "Epoch 213/1000 - Loss: 0.03158, Val Loss: 0.00084\n",
      "Epoch 214/1000 - Loss: 0.03147, Val Loss: 0.00081\n",
      "Epoch 215/1000 - Loss: 0.03135, Val Loss: 0.00078\n",
      "Epoch 216/1000 - Loss: 0.03125, Val Loss: 0.00076\n",
      "Epoch 217/1000 - Loss: 0.03114, Val Loss: 0.00074\n",
      "Epoch 218/1000 - Loss: 0.03103, Val Loss: 0.00071\n",
      "Epoch 219/1000 - Loss: 0.03092, Val Loss: 0.00070\n",
      "Epoch 220/1000 - Loss: 0.03082, Val Loss: 0.00068\n",
      "Epoch 221/1000 - Loss: 0.03072, Val Loss: 0.00067\n",
      "Epoch 222/1000 - Loss: 0.03061, Val Loss: 0.00065\n",
      "Epoch 223/1000 - Loss: 0.03051, Val Loss: 0.00064\n",
      "Epoch 224/1000 - Loss: 0.03041, Val Loss: 0.00064\n",
      "Epoch 225/1000 - Loss: 0.03031, Val Loss: 0.00063\n",
      "Epoch 226/1000 - Loss: 0.03021, Val Loss: 0.00063\n",
      "Epoch 227/1000 - Loss: 0.03011, Val Loss: 0.00063\n",
      "Epoch 228/1000 - Loss: 0.03002, Val Loss: 0.00063\n",
      "Epoch 229/1000 - Loss: 0.02992, Val Loss: 0.00063\n",
      "Epoch 230/1000 - Loss: 0.02983, Val Loss: 0.00064\n",
      "Epoch 231/1000 - Loss: 0.02973, Val Loss: 0.00064\n",
      "Epoch 232/1000 - Loss: 0.02964, Val Loss: 0.00065\n",
      "Epoch 233/1000 - Loss: 0.02955, Val Loss: 0.00066\n",
      "Epoch 234/1000 - Loss: 0.02946, Val Loss: 0.00067\n",
      "Epoch 235/1000 - Loss: 0.02937, Val Loss: 0.00069\n",
      "Epoch 236/1000 - Loss: 0.02928, Val Loss: 0.00070\n",
      "Epoch 237/1000 - Loss: 0.02919, Val Loss: 0.00072\n",
      "Epoch 238/1000 - Loss: 0.02910, Val Loss: 0.00074\n",
      "Epoch 239/1000 - Loss: 0.02901, Val Loss: 0.00076\n",
      "Epoch 240/1000 - Loss: 0.02893, Val Loss: 0.00079\n",
      "Epoch 241/1000 - Loss: 0.02884, Val Loss: 0.00081\n",
      "Epoch 242/1000 - Loss: 0.02876, Val Loss: 0.00084\n",
      "Epoch 243/1000 - Loss: 0.02868, Val Loss: 0.00086\n",
      "Epoch 244/1000 - Loss: 0.02859, Val Loss: 0.00089\n",
      "Epoch 245/1000 - Loss: 0.02851, Val Loss: 0.00092\n",
      "Epoch 246/1000 - Loss: 0.02843, Val Loss: 0.00096\n",
      "Epoch 247/1000 - Loss: 0.02835, Val Loss: 0.00099\n",
      "Epoch 248/1000 - Loss: 0.02827, Val Loss: 0.00103\n",
      "Epoch 249/1000 - Loss: 0.02819, Val Loss: 0.00106\n",
      "Epoch 250/1000 - Loss: 0.02812, Val Loss: 0.00110\n",
      "Epoch 251/1000 - Loss: 0.02804, Val Loss: 0.00114\n",
      "Epoch 252/1000 - Loss: 0.02796, Val Loss: 0.00118\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "trainer = LSTMTrainer(lstm, learning_rate=1e-3, patience=50, verbose=True, delta=0.001)\n",
    "trainer.train(trainX, trainY, testX, testY, epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ad1451-55f0-4d52-92c6-7ffb484141eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_manager = PlotManager()\n",
    "\n",
    "# Inside your training loop\n",
    "plot_manager.plot_losses(trainer.train_losses, trainer.val_losses)\n",
    "\n",
    "# After your training loop\n",
    "plot_manager.show_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5c6aa-9465-42c8-84f3-a0acf4189451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
