{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4909d6-9eed-450e-bb5c-7dfbe7575b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146453\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# List to store sentences\n",
    "all_sentences = []\n",
    "\n",
    "# Loop through files named '001ssb.txt' to '005ssb.txt'\n",
    "for i in range(1, 6):\n",
    "    # Create the filename\n",
    "    filename = f'dataset/{i:03d}ssb.txt'\n",
    "    \n",
    "    # Open the file and read the contents with error handling\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            # Read the file content\n",
    "            content = file.read()\n",
    "            \n",
    "            # Split the content into sentences (can be further customized as needed)\n",
    "            sentences = content.split('.')  # Assuming sentences are separated by periods\n",
    "            \n",
    "            # Tokenize each sentence using simple_preprocess\n",
    "            tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences if sentence]\n",
    "            \n",
    "            # Add the tokenized sentences to the list\n",
    "            all_sentences.extend(tokenized_sentences)\n",
    "    \n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "# Output the sentences for verification\n",
    "print(len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c572eea-7b39-4dd8-8df7-c31ef4f5c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'of', 'thrones', 'book', 'one', 'of', 'song', 'of', 'ice', 'and', 'fire', 'by', 'george']\n",
      "[]\n",
      "['martin', 'prologue', 'we', 'should', 'start', 'back', 'gared', 'urged', 'as', 'the', 'woods', 'began', 'to', 'grow', 'dark', 'around', 'them']\n"
     ]
    }
   ],
   "source": [
    "for sentence in all_sentences[:3]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9269956-7144-4b65-a9e8-1214446a4ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Remove stopwords from tokenized sentences\n",
    "cleaned_sentences = [[word for word in sentence if word not in STOPWORDS] for sentence in all_sentences]\n",
    "len(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6baed3-aae2-484b-9a81-15898b7a0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'thrones', 'book', 'song', 'ice', 'george']\n",
      "[]\n",
      "['martin', 'prologue', 'start', 'gared', 'urged', 'woods', 'began', 'grow', 'dark']\n"
     ]
    }
   ],
   "source": [
    "for sentence in cleaned_sentences[:3]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f69d17a-de26-4836-a3ce-16b24e157962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146453"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences = [\n",
    "    ['iron_throne' if word == 'iron' and idx + 1 < len(sentence) and sentence[idx + 1] == 'throne' \n",
    "     else word for idx, word in enumerate(sentence)] \n",
    "    for sentence in cleaned_sentences\n",
    "]\n",
    "len(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df09f06-d9fb-4545-89e9-01a4b28e7a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0014816516796277094\n",
      "Epoch 1/10, Loss: 0.0014816516796277094\n",
      "Epoch 2/10, Loss: 4.796326365560414e-07\n",
      "Epoch 2/10, Loss: 4.796326365560414e-07\n",
      "Epoch 3/10, Loss: 6.54142245294683e-08\n",
      "Epoch 3/10, Loss: 6.54142245294683e-08\n",
      "Epoch 4/10, Loss: 5.639256717651377e-08\n",
      "Epoch 4/10, Loss: 5.639256717651377e-08\n",
      "Epoch 5/10, Loss: 5.2225865847682225e-08\n",
      "Epoch 5/10, Loss: 5.2225865847682225e-08\n",
      "Epoch 6/10, Loss: 4.277900221733462e-08\n",
      "Epoch 6/10, Loss: 4.277900221733462e-08\n",
      "Epoch 7/10, Loss: 3.727926948139908e-08\n",
      "Epoch 7/10, Loss: 3.727926948139908e-08\n",
      "Epoch 8/10, Loss: 3.7280028312165856e-08\n",
      "Epoch 8/10, Loss: 3.7280028312165856e-08\n",
      "Epoch 9/10, Loss: 3.7283561960571904e-08\n",
      "Epoch 9/10, Loss: 3.7283561960571904e-08\n",
      "Epoch 10/10, Loss: 3.688039127838545e-08\n",
      "Epoch 10/10, Loss: 3.688039127838545e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Step 1: Tokenization\n",
    "# Build vocabulary and map words to indices\n",
    "def build_vocab(sentences):\n",
    "    all_words = list(chain.from_iterable(sentences))\n",
    "    word_counts = Counter(all_words)\n",
    "    vocab = {word: idx + 2 for idx, word in enumerate(word_counts)}  # +2 for padding and unknown\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "def tokenize_sentence(sentence, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in sentence]\n",
    "\n",
    "# Step 2: Create Dataset\n",
    "class GOTDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = [tokenize_sentence(sentence, vocab) for sentence in sentences]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx], dtype=torch.long)\n",
    "\n",
    "# Step 3: LSTM Model Definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        output = self.fc(hidden[-1])  # Use the last hidden state for classification\n",
    "        return output.view(-1, 1)  # Reshape to (batch_size, 1)\n",
    "\n",
    "# Step 4: Padding\n",
    "def collate_fn(batch):\n",
    "    # Pad the sequences to the length of the longest sequence in the batch\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return batch\n",
    "\n",
    "# Prepare data\n",
    "vocab = build_vocab(cleaned_sentences)\n",
    "dataset = GOTDataset(cleaned_sentences, vocab)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Step 5: Train the model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100  # Same as Word2Vec vector size\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # You can modify this based on your task\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification, change to CrossEntropyLoss for multi-class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        \n",
    "        # Assuming you have binary classification labels (0s and 1s)\n",
    "        labels = torch.ones_like(outputs)  # Replace this with your actual labels\n",
    "        labels = labels.view(-1, 1)  # Ensure labels have shape (batch_size, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c89527b-323a-47ac-ac87-32862bcee68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b43ee6d-912b-47b7-ba4d-d0f64ea09afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjon\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "print(len(model.embedding.weight.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f28ad8-502e-432e-b524-4a85096d72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jon\n",
      "None\n",
      "snow\n",
      "daenerys\n",
      "None\n",
      "targaryen\n",
      "tyrion\n",
      "None\n",
      "lannister\n",
      "cersei\n",
      "None\n",
      "lannister\n",
      "                 Name similarity      house\n",
      "0            Jon Snow       None       snow\n",
      "1  Daenerys Targaryen       None  targaryen\n",
      "2    Tyrion Lannister       None  lannister\n",
      "3    Cersei Lannister       None  lannister\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'model' is your trained LSTM model with an embedding layer\n",
    "# Create a vocabulary mapping from words to indices\n",
    "# This assumes 'model.embedding' is your embedding layer\n",
    "vocab_mapping = {word: idx for idx, word in enumerate(model.embedding.weight.data)}\n",
    "# Function to get embedding vector from the LSTM model\n",
    "def get_embedding(name):\n",
    "    # Convert name to index\n",
    "    name_idx = vocab_mapping.get(name)\n",
    "    print(name)\n",
    "    if name_idx is not None:  # Ensure the name exists in the vocabulary\n",
    "        input_tensor = torch.tensor([[name_idx]])  # Shape: (1, 1)\n",
    "        with torch.no_grad():\n",
    "            embedding_vector = model.embedding(input_tensor)\n",
    "        return embedding_vector.squeeze(0).squeeze(0)  # Shape: (embedding_dim)\n",
    "    return None  # Return None if the name is not found\n",
    "\n",
    "# Function to calculate similarity considering both first and last name using LSTM model\n",
    "def compute_similarity(name):\n",
    "    name_parts = name.split(\" \")\n",
    "    first_name = name_parts[0].lower()\n",
    "    last_name = name_parts[1].lower() if len(name_parts) > 1 else None\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Calculate similarity for the first name\n",
    "    first_name_embedding = get_embedding(first_name)\n",
    "    print(first_name_embedding)\n",
    "    if first_name_embedding is not None:\n",
    "        iron_throne_embedding = get_embedding('iron_throne')\n",
    "        if iron_throne_embedding is not None:\n",
    "            similarity = F.cosine_similarity(first_name_embedding, iron_throne_embedding, dim=0)\n",
    "            similarities.append(similarity.item())  # Convert tensor to float\n",
    "    # Calculate similarity for the last name (if it exists)\n",
    "    if last_name:\n",
    "        last_name_embedding = get_embedding(last_name)\n",
    "        if last_name_embedding is not None:\n",
    "            iron_throne_embedding = get_embedding('iron_throne')\n",
    "            if iron_throne_embedding is not None:\n",
    "                similarity = F.cosine_similarity(last_name_embedding, iron_throne_embedding, dim=0)\n",
    "                similarities.append(similarity.item())  # Convert tensor to float\n",
    "    \n",
    "    # Return the average similarity if both names exist, otherwise just one similarity\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    else:\n",
    "        return None  # Return None if no name part is in the vocabulary\n",
    "\n",
    "# Function to get last name\n",
    "def get_last_name(name):\n",
    "    name_parts = name.split(\" \")\n",
    "    last_name = name_parts[1].lower() if len(name_parts) > 1 else None\n",
    "    return last_name\n",
    "        \n",
    "# Sample DataFrame with character names (replace this with your actual DataFrame)\n",
    "characters = pd.DataFrame({'Name': ['Jon Snow', 'Daenerys Targaryen', 'Tyrion Lannister', 'Cersei Lannister']})\n",
    "\n",
    "# Apply the function to calculate similarity for each character\n",
    "characters['similarity'] = characters['Name'].apply(compute_similarity)\n",
    "\n",
    "# Apply the function to get last names for each character\n",
    "characters['house'] = characters['Name'].apply(get_last_name)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(characters.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10406a2a-9d53-4543-8da1-b117bef18f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.5620943e-01, -2.4885155e-01,  1.1386960e+00,  3.7612301e-01,\n",
       "       -2.6545791e-02, -1.4147528e+00,  8.2844299e-01, -4.3518519e-01,\n",
       "       -5.9370720e-01, -9.5779550e-01,  2.4015382e-01, -6.1506528e-01,\n",
       "        9.2488825e-03,  1.1070847e-01, -4.3834284e-01, -4.4569808e-01,\n",
       "       -3.6657352e-02, -2.8070518e-01, -9.3288469e-01, -1.0664799e+00,\n",
       "        1.7675240e-01,  7.1801430e-01,  5.5871081e-01, -1.3113753e+00,\n",
       "       -4.6027428e-01,  9.0649945e-01, -8.9804769e-01,  7.7424628e-01,\n",
       "       -8.0078834e-01,  6.5633863e-01, -2.1794173e-01,  2.5627020e-01,\n",
       "        5.9267098e-01, -1.7264159e+00, -8.7621361e-02,  1.1811959e-01,\n",
       "       -4.0051216e-01,  3.7502930e-01, -4.6692187e-01,  1.3635179e-01,\n",
       "        8.6818302e-01, -2.6004326e-01, -5.4952836e-01,  2.5428715e-01,\n",
       "       -7.0807226e-02, -1.3207475e+00, -7.0047897e-01, -2.3358734e-01,\n",
       "        8.0037642e-01,  8.6103618e-01, -3.6515000e-01, -6.9662023e-01,\n",
       "        1.0320263e+00, -8.9662910e-01, -4.1205952e-01,  1.3783135e+00,\n",
       "       -1.0801241e+00,  1.0789629e+00, -3.4422496e-01,  1.0920469e+00,\n",
       "        7.2091204e-01, -8.1593597e-01, -1.9235617e-01,  2.3708208e-01,\n",
       "       -6.9956705e-02,  6.8194652e-01,  2.9017904e-01,  8.8063622e-01,\n",
       "       -1.7480021e-04, -1.0163347e+00,  4.1505039e-01,  9.4692193e-02,\n",
       "        7.8717440e-01,  7.5169533e-01, -1.7371316e-01, -1.2152547e-01,\n",
       "        3.7115419e-01, -5.6137085e-01,  4.8789361e-01, -1.0382905e+00,\n",
       "       -1.1410937e+00,  1.0869089e+00, -6.0632008e-01,  2.6286602e-01,\n",
       "        6.3058919e-01, -8.1633776e-01,  3.5296139e-01,  6.5903544e-01,\n",
       "        1.5840632e+00,  2.7912259e-01,  1.1011857e+00,  4.5929229e-01,\n",
       "        2.0983122e-02,  4.1445678e-01, -6.6025496e-01, -9.7687595e-02,\n",
       "       -7.2073793e-01,  3.6123091e-01,  3.1375837e-02,  1.9872957e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.wv['jon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44cd0886-f05c-4a1f-aa90-6d3a2df49d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3909363e-01, -2.2002006e-01,  4.7506067e-01,  7.7871668e-01,\n",
       "        5.3651970e-02, -5.2936649e-01,  1.5886328e+00,  2.0001726e-01,\n",
       "       -1.2297843e+00,  9.4697887e-01, -1.5030047e-01, -9.2730635e-01,\n",
       "        3.9230552e-02,  4.4592194e-02,  3.9217886e-02, -1.3617882e+00,\n",
       "        8.2519382e-02, -4.6770964e-02, -6.2201005e-01, -1.3790336e+00,\n",
       "        8.3843723e-02, -9.9067819e-01,  1.0594131e+00,  2.1284914e-01,\n",
       "       -7.4863583e-02,  6.1514056e-01, -6.4188844e-01, -6.2276477e-01,\n",
       "       -1.7749134e-01,  2.2741379e-01, -6.9773555e-02, -5.3160846e-01,\n",
       "        6.9203397e-04,  7.8685653e-01,  1.2047927e+00,  1.0464323e+00,\n",
       "       -3.2411852e-01,  6.0192398e-03, -1.3213059e+00, -1.3241209e+00,\n",
       "        5.8795843e-02, -1.0450821e+00, -1.1407553e+00,  2.7123210e-01,\n",
       "        4.2258427e-01, -6.0977143e-01, -3.1508815e-01,  4.4198966e-01,\n",
       "        8.4960675e-01,  6.7589730e-01,  2.7892965e-01, -5.4389244e-01,\n",
       "       -9.2235573e-02,  6.5729505e-01, -1.1411744e+00, -3.4988809e-01,\n",
       "       -3.1922840e-02,  1.6246520e-01, -1.3406752e+00,  6.5425837e-01,\n",
       "        8.1071593e-02, -8.2377779e-01,  1.0381087e-01,  3.4699374e-01,\n",
       "       -1.2464025e+00,  1.7005734e-01, -7.9446606e-02, -6.6123977e-02,\n",
       "       -3.4910461e-01,  3.9234638e-01, -5.0205278e-01,  5.4959875e-01,\n",
       "        1.1024417e+00, -2.2376046e-01,  3.3475074e-01,  7.7691215e-01,\n",
       "       -4.9144391e-02, -1.5671051e-01,  2.6700428e-01, -6.4097971e-01,\n",
       "       -3.7507638e-01, -9.1254091e-01,  4.5126763e-01,  3.2125139e-01,\n",
       "        1.5903965e-01, -2.0422132e-01,  5.4510742e-01,  2.3441118e-01,\n",
       "        4.5788527e-01,  1.3809006e-01,  3.0317128e-01, -2.0308606e-01,\n",
       "        6.6063154e-01,  6.6635936e-01,  3.1380194e-01,  7.5141454e-01,\n",
       "        1.3165269e+00, -6.4931923e-01,  1.4946139e-01,  4.5119217e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_king_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "426652f7-5a9a-464d-93c6-0506dd1891a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words to 'king' in the CBOW model\n",
    "similar_words_cbow = model_cbow.wv.most_similar(positive=[vector_king_cbow], topn=10)\n",
    "\n",
    "# Find most similar words to 'king' in the Skip-Gram model\n",
    "similar_words_skipgram = model_skipgram.wv.most_similar(positive=[vector_king_skipgram], topn=10)\n",
    "\n",
    "# Print the similar words along with their similarity scores\n",
    "#print(\"Similar words to 'iron throne' in CBOW model:\", similar_words_cbow)\n",
    "#print(\"Similar words to 'iron throne' in Skip-Gram model:\", similar_words_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed6e3e17-cfc0-460a-85e6-d2840747d167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.381342"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score = model_cbow.wv.similarity('iron_throne', 'stark')\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ad9abae-6e4a-4517-9139-a00a47c7b97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addam Marbrand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aegon Frey (Jinglebell)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aegon Targaryen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adrack Humble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aemon Costayne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name\n",
       "0           Addam Marbrand\n",
       "1  Aegon Frey (Jinglebell)\n",
       "2          Aegon Targaryen\n",
       "3            Adrack Humble\n",
       "4           Aemon Costayne"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "characters = pd.read_csv('dataset/character-deaths.csv')[[\"Name\"]]\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cfb4606b-4686-4c27-9123-bbead2213cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marbrand'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters['Name'].iloc[0].split(\" \")[1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82a50f24-99f7-450b-ad78-df7ca4fe63fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Name  similarity      house\n",
      "0           Addam Marbrand    0.464045   marbrand\n",
      "1  Aegon Frey (Jinglebell)    0.613120       frey\n",
      "2          Aegon Targaryen    0.744675  targaryen\n",
      "3            Adrack Humble    0.561879     humble\n",
      "4           Aemon Costayne    0.492332   costayne\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate similarity considering both first and last name\n",
    "def compute_similarity(name):\n",
    "    name_parts = name.split(\" \")\n",
    "    first_name = name_parts[0].lower()\n",
    "    last_name = name_parts[1].lower() if len(name_parts) > 1 else None\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Calculate similarity for the first name\n",
    "    if first_name in model_cbow.wv:\n",
    "        similarities.append(model_cbow.wv.similarity('iron_throne', first_name))\n",
    "    \n",
    "    # Calculate similarity for the last name (if it exists)\n",
    "    if last_name and last_name in model_cbow.wv:\n",
    "        similarities.append(model_cbow.wv.similarity('iron_throne', last_name))\n",
    "    \n",
    "    # Return the average similarity if both names exist, otherwise just one similarity\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    else:\n",
    "        return None  # Return None if no name part is in the vocabulary\n",
    "\n",
    "# Function to calculate similarity considering both first and last name\n",
    "def get_last_name(name):\n",
    "    name_parts = name.split(\" \")\n",
    "    last_name = name_parts[1].lower() if len(name_parts) > 1 else None\n",
    "    return last_name\n",
    "        \n",
    "# Apply the function to calculate similarity for each character\n",
    "characters['similarity'] = characters['Name'].apply(compute_similarity)\n",
    "\n",
    "# Apply the function to calculate similarity for each character\n",
    "characters['house'] = characters['Name'].apply(get_last_name)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(characters.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d3e51b6-5c99-4b09-8637-068b5a844f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addam Marbrand</td>\n",
       "      <td>0.464045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aegon Frey (Jinglebell)</td>\n",
       "      <td>0.613120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aegon Targaryen</td>\n",
       "      <td>0.744675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adrack Humble</td>\n",
       "      <td>0.561879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  similarity\n",
       "0           Addam Marbrand    0.464045\n",
       "1  Aegon Frey (Jinglebell)    0.613120\n",
       "2          Aegon Targaryen    0.744675\n",
       "3            Adrack Humble    0.561879"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b3975ed3-31aa-49c3-bd5d-ebe99fa59eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>similarity</th>\n",
       "      <th>house</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aegon Targaryen</td>\n",
       "      <td>0.744675</td>\n",
       "      <td>targaryen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>Renly Baratheon</td>\n",
       "      <td>0.717126</td>\n",
       "      <td>baratheon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>Robert Baratheon</td>\n",
       "      <td>0.716023</td>\n",
       "      <td>baratheon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Joffrey Baratheon</td>\n",
       "      <td>0.712842</td>\n",
       "      <td>baratheon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>Zei</td>\n",
       "      <td>0.707267</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Jack-Be-Lucky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Jommy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Three-Tooth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>Will</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Will (orphan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(orphan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>917 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name  similarity      house\n",
       "2      Aegon Targaryen    0.744675  targaryen\n",
       "689    Renly Baratheon    0.717126  baratheon\n",
       "702   Robert Baratheon    0.716023  baratheon\n",
       "411  Joffrey Baratheon    0.712842  baratheon\n",
       "911                Zei    0.707267       None\n",
       "..                 ...         ...        ...\n",
       "379      Jack-Be-Lucky         NaN       None\n",
       "405              Jommy         NaN       None\n",
       "805        Three-Tooth         NaN       None\n",
       "881               Will         NaN       None\n",
       "882      Will (orphan)         NaN   (orphan)\n",
       "\n",
       "[917 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame by 'similarity' column in descending order\n",
    "characters_sorted = characters.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "\n",
    "characters_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e0f3adf-4298-4caf-a8ef-167fb4214e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_sorted.to_csv('sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fb995-02b0-408b-b1cf-a7e16b709710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
